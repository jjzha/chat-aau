services:
  vllm:
    image: vllm/vllm-openai:v0.8.5
    ports:
      - "8000:8000"
    runtime: nvidia
    ipc: host
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} # Make sure HF_TOKEN is set in your environment or a .env file
    command: [
      "--model=Qwen/Qwen3-4B",
      "--dtype=bfloat16",
      "--tensor-parallel-size=2",
      "--trust-remote-code",
      "--max-model-len=8192",
      "--gpu-memory-utilization=0.95",
      "--enable-reasoning",
      "--reasoning-parser=deepseek_r1",
      "--api_key=aau-1"
    ]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    restart: always

  open-webui:
    # image: ghcr.io/open-webui/open-webui:latest
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - OPENAI_API_BASE_URL=http://vllm:8000/v1
      - OPENAI_API_KEY=aau-1
      - ENABLE_OLLAMA_API=false
    ports:
      - "3000:8080"
    depends_on:
      - vllm
    volumes:
      - open-webui:/app/backend/data
    restart: always

  tika:
    image: apache/tika:latest-full
    container_name: tika
    ports:
      - "9998:9998"
    restart: unless-stopped

volumes:
  open-webui: {}