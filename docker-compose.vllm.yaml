services:
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    runtime: nvidia
    ipc: host
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: [
      "--model=google/gemma-3-4b-it",
      "--dtype=bfloat16",
      "--tensor-parallel-size=2",
      "--trust-remote-code",
      "--max-model-len=8192",
      "--gpu-memory-utilization=0.95",
      "--api_key=aau-1"
    ]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    restart: always

  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    environment:
      - OPENAI_API_BASE_URL=http://130.225.37.49:8000/v1 # Or keep the specific IP if needed
      - OPENAI_API_KEY=aau-1
      - ENABLE_OLLAMA_API=false
    ports:
      - "3000:8080" # Map host port 3000 to container port 8080
    depends_on:
      - vllm # Ensures vllm starts before open-webui
    volumes:
      - open-webui:/app/backend/data
    restart: always

  tika:
    image: apache/tika:latest-full
    container_name: tika
    ports:
      - "9998:9998"
    restart: unless-stopped
